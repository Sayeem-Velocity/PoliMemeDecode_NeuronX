{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3SXsREFdtf_"
      },
      "source": [
        "# **Multilingual Meme OCR Pipeline**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Project Overview\n",
        "\n",
        "This notebook implements an Optical Character Recognition (OCR) pipeline specifically optimized for meme content containing Bangla, English, and Banglish (Bengali text written in Roman script). It utilizes a fine-tuned Vision-Language Model (VLM) to extract text and a heuristic-based classifier to determine the language.\n",
        "\n",
        "# Key Features\n",
        "**Model:** nanonets/Nanonets-OCR-s (Based on Qwen2.5-VL).\n",
        "\n",
        "**Optimization:** Uses 4-bit Quantization (NF4) via bitsandbytes to run efficiently on Kaggle's Tesla T4 (16GB VRAM) without Out-Of-Memory (OOM) errors.\n",
        "\n",
        "**Memory Management:** Implements aggressive garbage collection and resolution capping (max 768px) to maintain stability during batch processing.\n",
        "\n",
        "**Language Classification:** Custom logic to distinguish between Bangla (Unicode), English (Dictionary check), and Banglish.\n",
        "\n",
        "# Output\n",
        "The notebook generates a CSV file with the following columns:\n",
        "- **extracted_text**: The text extracted from the meme image using OCR.\n",
        "- **Language**: The detected language classification (Bangla, English, or Banglish).\n",
        "\n",
        "# Pipeline Architecture\n",
        "1. Environment Setup\n",
        "The notebook installs specific versions of transformers and qwen-vl-utils to support the Qwen2.5-VL architecture.\n",
        "\n",
        "Libraries: torch, transformers, accelerate, bitsandbytes, pillow.\n",
        "\n",
        "2. Language Classification Logic\n",
        "A custom function classify_language(text) categorizes the extracted text:\n",
        "\n",
        "Bangla: Detected via Unicode range (\\u0980 to \\u09FF).\n",
        "\n",
        "English: Detected by calculating the density of common English stop words against the total word count.\n",
        "\n",
        "Banglish: If the text uses Roman characters but lacks sufficient English vocabulary density, it is classified as Banglish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-12-01T15:08:16.301327Z",
          "iopub.status.busy": "2025-12-01T15:08:16.301094Z",
          "iopub.status.idle": "2025-12-01T15:10:11.688638Z",
          "shell.execute_reply": "2025-12-01T15:10:11.687933Z",
          "shell.execute_reply.started": "2025-12-01T15:08:16.301309Z"
        },
        "id": "SYoaG3YZdklU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git accelerate qwen-vl-utils pandas torch pillow bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-01T15:10:11.690988Z",
          "iopub.status.busy": "2025-12-01T15:10:11.690718Z",
          "iopub.status.idle": "2025-12-01T16:18:34.311111Z",
          "shell.execute_reply": "2025-12-01T16:18:34.310430Z",
          "shell.execute_reply.started": "2025-12-01T15:10:11.690962Z"
        },
        "id": "VdkQmbDBdklV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "IMAGE_DIR = \"/kaggle/input/poli-meme-decode-cuet-cse-fest/PoliMemeDecode/Test/Image\"\n",
        "OUTPUT_DIR = \"/kaggle/working\"\n",
        "MODEL_PATH = \"nanonets/Nanonets-OCR-s\"\n",
        "\n",
        "# ======================================================\n",
        "# 1. LANGUAGE CLASSIFIER (Same as before)\n",
        "# ======================================================\n",
        "def classify_language(text):\n",
        "    if not text or len(text.strip()) < 2: return \"Unknown\"\n",
        "    if any('\\u0980' <= char <= '\\u09FF' for char in text): return \"Bangla\"\n",
        "    common_english = {'the', 'is', 'a', 'an', 'and', 'to', 'in', 'of', 'for', 'it', 'you', 'me', 'he', 'she', 'that', 'this', 'what', 'when', 'why', 'how', 'good', 'bad', 'day', 'night', 'lol', 'pov', 'bro', 'meme'}\n",
        "    words = [w.lower().strip(\".,!?\\\"'\") for w in text.split()]\n",
        "    if not words: return \"Unknown\"\n",
        "    english_word_count = sum(1 for w in words if w in common_english)\n",
        "    if (english_word_count / len(words)) < 0.2: return \"Banglish\"\n",
        "    return \"English\"\n",
        "\n",
        "# ======================================================\n",
        "# 2. LOAD MODEL IN 4-BIT MODE (The Fix for OOM)\n",
        "# ======================================================\n",
        "print(\"Loading Model in 4-bit quantization...\")\n",
        "\n",
        "# This config reduces VRAM usage from ~7GB to ~2.5GB\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "try:\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        MODEL_PATH,\n",
        "        quantization_config=bnb_config, # <--- 4-bit magic\n",
        "        attn_implementation=\"eager\",    # <--- T4 safe attention\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
        "    print(\"Model loaded! VRAM usage should be very low now.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Load Error: {e}\")\n",
        "    raise e\n",
        "\n",
        "# ======================================================\n",
        "# 3. PROCESSING LOOP WITH RESOLUTION LIMITS\n",
        "# ======================================================\n",
        "def process_image(image_path):\n",
        "    try:\n",
        "        # Load image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # FIX: Force resize to max 768px (Prevents 13GB allocations)\n",
        "        # This is high enough for memes but low enough for T4 GPU\n",
        "        max_dimension = 768\n",
        "        if max(image.size) > max_dimension:\n",
        "            image.thumbnail((max_dimension, max_dimension), Image.LANCZOS)\n",
        "\n",
        "        prompt = \"Extract text from this meme. If it is Banglish, write it exactly as shown. Do not translate.\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": prompt}]}\n",
        "        ]\n",
        "\n",
        "        # Prepare inputs\n",
        "        text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = processor(text=[text_input], images=[image], padding=True, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=512,\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "        generated_ids_trimmed = [\n",
        "            out_ids[len(in_ids):]\n",
        "            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "\n",
        "        output_text = processor.batch_decode(\n",
        "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "        )[0]\n",
        "\n",
        "        # AGGRESSIVE CLEANUP\n",
        "        del inputs, generated_ids, image\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return output_text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error on {image_path.name}: {str(e)[:100]}\")\n",
        "        torch.cuda.empty_cache()\n",
        "        return \"\"\n",
        "\n",
        "# ======================================================\n",
        "# 4. EXECUTION\n",
        "# ======================================================\n",
        "image_files = sorted([p for p in Path(IMAGE_DIR).rglob('*') if p.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']])\n",
        "print(f\"Found {len(image_files)} images\")\n",
        "\n",
        "results = []\n",
        "checkpoint_path = f\"{OUTPUT_DIR}/checkpoint.csv\"\n",
        "\n",
        "for i, img_path in enumerate(tqdm(image_files)):\n",
        "    text = process_image(img_path)\n",
        "    lang = classify_language(text)\n",
        "\n",
        "    results.append({\n",
        "        \"image_filename\": img_path.name,\n",
        "        \"extracted_text\": text,\n",
        "        \"Language\": lang\n",
        "    })\n",
        "\n",
        "    # Save frequently\n",
        "    if i % 10 == 0:\n",
        "        pd.DataFrame(results).to_csv(checkpoint_path, index=False)\n",
        "        gc.collect()\n",
        "\n",
        "# Final Save\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(f\"{OUTPUT_DIR}/final_meme_texts.csv\", index=False, encoding='utf-8-sig')\n",
        "print(f\"Finished! Saved to {OUTPUT_DIR}/final_meme_texts.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 14664749,
          "sourceId": 124686,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
